{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHsUJmMB0X7eha3nl9PWNs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Language Image Pre-Training (CLIP) on Radiology Objects in COntext (ROCO)"
      ],
      "metadata": {
        "id": "lGtgnSaFPORr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and data\n",
        "Before starting executing the notebook, do the following steps:\n",
        "- Go to \"Runtime\" > \"Change type of runtime\" and select a GPU-based runtime;\n",
        "- Load the `resized_train.zip`, `caption_prediction_train.csv`, `concept_detection_train.csv`  files in the File folder."
      ],
      "metadata": {
        "id": "XbTjR7azPWUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7QRfnMo0PMnC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tfk.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the random seed for reproducibility:"
      ],
      "metadata": {
        "id": "u-GUnZwNarmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 24948989491\n",
        "\n",
        "rng = np.random.default_rng(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "FNGjCsktauhq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the images from `resized_train.zip`:"
      ],
      "metadata": {
        "id": "iiuBdadSXatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip resized_train.zip"
      ],
      "metadata": {
        "id": "tpt7w_QBZEis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load textual data:"
      ],
      "metadata": {
        "id": "FcAwDGgbb_yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the captions\n",
        "captions = pd.read_csv(\"caption_prediction_train.csv\",sep=\"\\t\", index_col=\"ID\")\n",
        "\n",
        "# Load the labels\n",
        "labels = pd.read_csv(\"concept_detection_train.csv\",sep=\"\\t\",index_col=\"ID\")\n",
        "# Each label string contains multiple labels separated by a semicolumn.\n",
        "# Transform the strings in lists of labels.\n",
        "labels[\"cuis\"] = labels[\"cuis\"].str.split(pat=\";\")"
      ],
      "metadata": {
        "id": "VvlLdp2XbsKw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explore the images to check if they are all 128x128 pixel and how many channels they have:"
      ],
      "metadata": {
        "id": "KfsCqAbAmLlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_channels = 1\n",
        "for i in labels.index:\n",
        "  image_name = \"resized_train/\"+i+\".jpg\"\n",
        "  img = Image.open(image_name)\n",
        "  img = np.array(img)\n",
        "  if img.ndim==3 and img.shape[2] > max_channels:\n",
        "    max_channels = img.shape[2]\n",
        "  # Check if the image has the expected resolution of 128x128\n",
        "  if img.shape[0]!=128 or img.shape[1]!=128:\n",
        "    print(f\"Error in {image_name}: its resolution is \" +\n",
        "      f\"{img.shape[0]}x{img.shape[1]}, while it should be 128x128\")\n",
        "\n",
        "print(f\"The maximum number of channels is {max_channels}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvV_jUVokiPh",
        "outputId": "6bd730b1-3941-4d8e-db96-06a319ff98dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum number of channels is 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import all the images, we shuffle them and we split them in training and test sets:"
      ],
      "metadata": {
        "id": "yVU4R6F3mTSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.zeros((labels.shape[0],128,128,3),dtype=\"uint8\")\n",
        "count = 0\n",
        "for i in labels.index:\n",
        "  image_name = \"resized_train/\"+i+\".jpg\"\n",
        "  img = tf.io.read_file(image_name)\n",
        "  images[count]= tf.io.decode_image(img, channels=3)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "id": "vS2pyxPnOwju"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = np.sort(rng.choice(range(labels.shape[0]),\n",
        "                               size=int(0.8*labels.shape[0]), replace=False))\n",
        "test_idx = np.delete(range(labels.shape[0]), train_idx)\n",
        "\n",
        "images_train = images[train_idx]\n",
        "images_test = images[test_idx]\n",
        "# Delete the variable to free a lot of space in the RAM\n",
        "images=None"
      ],
      "metadata": {
        "id": "UQW6m7LlPxRy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the division of the images, we split also the labels and the captions in training and test sets."
      ],
      "metadata": {
        "id": "ZedmmRO66o2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions_train = captions.iloc[train_idx]\n",
        "captions_test = captions.iloc[test_idx]\n",
        "labels_train = labels.iloc[train_idx]\n",
        "labels_test = labels.iloc[test_idx]"
      ],
      "metadata": {
        "id": "W0dZ5ftm8let"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the CLIP model"
      ],
      "metadata": {
        "id": "SUzdpCGjbK0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the constants for the CLIP model."
      ],
      "metadata": {
        "id": "Rg4svRWmyaHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size of the images in input.\n",
        "input_shape = (128,128,3)\n",
        "# The vocabulary size for the words in the captions.\n",
        "vocab_size = 20000\n",
        "# The maximum number of words in a caption. Captions with a smaller number of\n",
        "# words are padded in the TextVectorization layer.\n",
        "sequence_length = 400\n",
        "# Dimension of the vectors representing each caption after the embedding.\n",
        "embed_dim = 128\n",
        "# Number of attention heads in each attention block.\n",
        "num_heads = 4\n",
        "# Dimension of the central layer of the feed forward NN.\n",
        "latent_dim = 1024\n",
        "# The final embedding dimensions for both texts and images\n",
        "final_embed_dim = 128"
      ],
      "metadata": {
        "id": "BJLzIE-ZoAlG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CNN"
      ],
      "metadata": {
        "id": "_iXoxq0hesIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the convolutional block, which will be the base block for the CNN."
      ],
      "metadata": {
        "id": "h4O0ayBYoOGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_block(x, filters, kernel):\n",
        "  '''\n",
        "  A function building a convolutional block in a neural network, composed of a\n",
        "  convolutional layer and a max pooling layer. It takes as parameters the number\n",
        "  of filters for the convolutional operation and the size of the convolutional\n",
        "  kernel.\n",
        "\n",
        "  Input shape: (None, h, w, channels)\n",
        "  Output shape: (None, h/2, w/2, filters)\n",
        "  '''\n",
        "  # Apply convolution operation to the input x\n",
        "  x = tfkl.Conv2D(\n",
        "      filters,\n",
        "      kernel,\n",
        "      strides=1,\n",
        "      padding='same',\n",
        "      activation='relu'\n",
        "  )(x)\n",
        "  # Apply max pooling operation to the convolution output\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "kwMLtYYUnJGO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the convolutional neural network, which is composed of a series of convolutional blocks and a flatten block."
      ],
      "metadata": {
        "id": "WBVToquMqrD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_convolutional_neural_network(blocks, filters, kernel):\n",
        "  '''\n",
        "  A function generating a Keras Model of a Convolutional Neural Network (CNN)\n",
        "  with the specified number of convolutional blocks, number of filters for each\n",
        "  convolutional block and kernel size.\n",
        "\n",
        "  Input shape: (None, h, w, channels)\n",
        "  Output shape: (None, h/(2^blocks)*w/(2^blocks)*filters*blocks)\n",
        "  '''\n",
        "  inputs = tfkl.Input(shape=input_shape, name='CNN_input')\n",
        "  x = tfkl.Normalization(name=\"CNN_normalization\")(inputs)\n",
        "  # Extract features via convolution and pooling operations\n",
        "  for b in range(blocks):\n",
        "      x = convolutional_block(x, filters*(b+1), kernel)\n",
        "  x = tfkl.Flatten(name=\"CNN_flatten\")(x)\n",
        "  # Create the Keras model\n",
        "  model = tfk.Model(inputs=inputs, outputs=x, name=\"Convolutional NN\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "EESmkOCuqq43"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Text Transformer"
      ],
      "metadata": {
        "id": "ZTPknBaqyJTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(tfkl.Layer):\n",
        "  '''\n",
        "  A custom Keras Layer implementing the token and position embedding block.\n",
        "  It consists of two embedding layers, one for the tokens and one for the\n",
        "  position of the token.\n",
        "  '''\n",
        "  def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Embedding layer for the token\n",
        "    self.token_emb = tfkl.Embedding(\n",
        "        input_dim=vocab_size, output_dim=embed_dim)\n",
        "    # Embedding layer for the position\n",
        "    self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "  def call(self, x):\n",
        "    '''\n",
        "    Input shape: (None, sequence_length)\n",
        "    Output shape: (None, sequence_length, embed_dim)\n",
        "    '''\n",
        "    # Create a tensor with positions from 0 to maxlen-1\n",
        "    positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
        "    # Embed the positions\n",
        "    positions = self.pos_emb(positions)\n",
        "    # Embed the tokens\n",
        "    x = self.token_emb(x)\n",
        "    # Add the token and position embeddings\n",
        "    return x + positions"
      ],
      "metadata": {
        "id": "RPhv5-8s2Dym"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(tfkl.Layer):\n",
        "  '''\n",
        "  A custom Keras Layer implementing a block in the Transformer encoder\n",
        "  architecture.\n",
        "  It consists of:\n",
        "  - Multi-head self-attention layer\n",
        "  - Feed-forward network with two dense layers and ReLU activation\n",
        "  - Two normalization layers\n",
        "  - Two dropout layers\n",
        "  '''\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.att =tfkl.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
        "    self.ffn = tfk.Sequential(\n",
        "        [\n",
        "            tfkl.Dense(ff_dim, activation=\"relu\"),\n",
        "            tfkl.Dense(embed_dim)\n",
        "        ]\n",
        "    )\n",
        "    self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = tfkl.Dropout(rate)\n",
        "    self.dropout2 = tfkl.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    '''\n",
        "    Input shape: (None, sequence_length, embed_dim)\n",
        "    Output shape: (None, sequence_length, embed_dim)\n",
        "    '''\n",
        "    # Self-attention\n",
        "    attn_output = self.att(inputs, inputs)\n",
        "    # Apply dropout to the attention output\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    # Add the attention output to the input and normalize\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    # Feed-forward\n",
        "    ffn_output = self.ffn(out1)\n",
        "    # Apply dropout to the feed-forward output\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    # Add the feed-forward output to the previous output and normalize\n",
        "    return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "F6yzyQzl2WAM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_neural_network(blocks):\n",
        "  '''\n",
        "  A function generating a Keras Model of a transfomer encoder with a specified\n",
        "  number of blocks.\n",
        "\n",
        "  Input shape: (None, sequence_length)\n",
        "  Output shape: (None, embed_dim)\n",
        "  '''\n",
        "  inputs = tfkl.Input(shape=(sequence_length), name='transformer_input')\n",
        "  x = TokenAndPositionEmbedding(sequence_length, vocab_size, embed_dim,\n",
        "                                name=\"token_position_embedding\")(inputs)\n",
        "  for i in range(blocks):\n",
        "    x = TransformerEncoderBlock(embed_dim, num_heads, latent_dim,\n",
        "                                name=\"transformer_encoder_block\"+str(i+1))(x)\n",
        "  x = tfkl.GlobalAveragePooling1D(name=\"transformer_global_average_pooling\")(x)\n",
        "  model = tfk.Model(inputs=inputs, outputs=x, name=\"Transformer\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "XM55P87q3H6C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CLIP Model"
      ],
      "metadata": {
        "id": "Zcpf44ijOon4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPConnector(tfkl.Layer):\n",
        "  '''\n",
        "  A custom Keras Layer for connecting the outputs of a CNN and a Transformer\n",
        "  model using the CLIP similarity computation.\n",
        "  '''\n",
        "  def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.t = self.add_weight(name=\"temperature\", trainable=True)\n",
        "        self.CNN_dense = tfkl.Dense(final_embed_dim, activation=None,\n",
        "                                    name=\"CNN_dense\")\n",
        "        self.transformer_dense = tfkl.Dense(final_embed_dim, activation=None,\n",
        "                                            name=\"transformer_dense\")\n",
        "\n",
        "  def call(self, CNN_output, transformer_output):\n",
        "    '''\n",
        "    Input shape:\n",
        "    - CNN_output: (None, CNN_output_size)\n",
        "    - transformer_output: (None, embed_dim)\n",
        "    Output shape: (None, None)\n",
        "    '''\n",
        "    # Use Dense layers to obtain the same embedding dimensions for the CNN and\n",
        "    # transformer encodings.\n",
        "    CNN_output_final_embed = self.CNN_dense(CNN_output)\n",
        "    transformer_output_final_embed = self.transformer_dense(transformer_output)\n",
        "\n",
        "    # l2 normalization of both embeddings.\n",
        "    CNN_output_final_embed_norm = tf.math.l2_normalize(\n",
        "        CNN_output_final_embed, axis=-1)\n",
        "    transformer_output_final_embed_norm = tf.math.l2_normalize(\n",
        "        transformer_output_final_embed, axis=-1)\n",
        "\n",
        "    # Computation of the cosine distance between the embeddings and\n",
        "    # multiplication for a trainable temperature parameter.\n",
        "    similarity_matrix = tf.linalg.matmul(transformer_output_final_embed_norm,\n",
        "                                         CNN_output_final_embed_norm,\n",
        "                                         transpose_a=False,\n",
        "                                         transpose_b=True)*tf.math.exp(self.t)\n",
        "\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "oD70AQNdxnwe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN = get_convolutional_neural_network(blocks=3, filters=64, kernel=5)\n",
        "transformer = get_transformer_neural_network(blocks=3)\n",
        "\n",
        "similarity_matrix = CLIPConnector(name=\"CLIP_connector\")(\n",
        "    CNN.output, transformer.output)\n",
        "\n",
        "CLIP=tfk.Model(inputs=[CNN.input, transformer.input], outputs=similarity_matrix,\n",
        "                 name=\"CLIP\")\n",
        "CLIP.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv7QKbccOxPW",
        "outputId": "5f8430b2-0323-4d60-e382-56ff73f5df93"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CLIP\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " CNN_input (InputLayer)         [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " CNN_normalization (Normalizati  (None, 128, 128, 3)  7          ['CNN_input[0][0]']              \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 128, 128, 64  4864        ['CNN_normalization[0][0]']      \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 64, 64, 64)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " transformer_input (InputLayer)  [(None, 400)]       0           []                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 64, 64, 128)  204928      ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " token_position_embedding (Toke  (None, 400, 128)    2611200     ['transformer_input[0][0]']      \n",
            " nAndPositionEmbedding)                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0          ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " transformer_encoder_block1 (Tr  (None, 400, 128)    527616      ['token_position_embedding[0][0]'\n",
            " ansformerEncoderBlock)                                          ]                                \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 192)  614592      ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_encoder_block2 (Tr  (None, 400, 128)    527616      ['transformer_encoder_block1[0][0\n",
            " ansformerEncoderBlock)                                          ]']                              \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 192)  0          ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " transformer_encoder_block3 (Tr  (None, 400, 128)    527616      ['transformer_encoder_block2[0][0\n",
            " ansformerEncoderBlock)                                          ]']                              \n",
            "                                                                                                  \n",
            " CNN_flatten (Flatten)          (None, 49152)        0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " transformer_global_average_poo  (None, 128)         0           ['transformer_encoder_block3[0][0\n",
            " ling (GlobalAveragePooling1D)                                   ]']                              \n",
            "                                                                                                  \n",
            " CLIP_connector (CLIPConnector)  (None, None)        6308097     ['CNN_flatten[0][0]',            \n",
            "                                                                  'transformer_global_average_pool\n",
            "                                                                 ing[0][0]']                      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,326,536\n",
            "Trainable params: 11,326,529\n",
            "Non-trainable params: 7\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfk.utils.plot_model(CLIP)"
      ],
      "metadata": {
        "id": "WqWq_MpMoLwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation"
      ],
      "metadata": {
        "id": "NPxUHE63REdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "optimizer = tfk.optimizers.Adam(learning_rate)\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "SrC9ZuY0H55g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the custom loss function"
      ],
      "metadata": {
        "id": "JXwbjplrRGzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CLIP_loss_fn(y_true, y_pred):\n",
        "  '''\n",
        "  Custom loss function for the CLIP model. It computes the similarity between\n",
        "  the output matrix of CLIP and the identity matrix. To do this, it exploits the\n",
        "  cross-entropy loss function on both axes of the output matrix and then\n",
        "  averages the two results.\n",
        "  '''\n",
        "  labels = tf.range(tf.shape(y_true)[0])\n",
        "  loss_function = tfk.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tfk.losses.Reduction.SUM)\n",
        "  loss_1 = loss_function(labels, y_pred)\n",
        "  loss_2 = loss_function(labels, tf.transpose(y_pred))\n",
        "  return (loss_1+loss_2)/2"
      ],
      "metadata": {
        "id": "IX_HfJNkRI4-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "HorzfjTqpYcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This code is performing text vectorization in TensorFlow's Keras library\n",
        "(tf.keras or tfk). It defines a TextVectorization layer with a vocabulary\n",
        "size of vocab_size, meaning it will only consider the top vocab_size most common\n",
        "tokens in the input text data. The layer has an output mode of \"int\",\n",
        "meaning it will return the vectorized representation of the text as integer\n",
        "token indices. The output sequence length is set to sequence_length, meaning\n",
        "each input text will be padded or truncated to sequence_length tokens.\n",
        "The adapt method is called on the training captions, allowing the vectorization\n",
        "layer to learn and update the tokenization and vocabulary mapping.\n",
        "'''\n",
        "vectorization = tfkl.TextVectorization(max_tokens=vocab_size, output_mode=\"int\",\n",
        "                                       output_sequence_length=sequence_length)\n",
        "vectorization.adapt(captions)\n",
        "captions_train_vect = vectorization(captions_train)\n",
        "captions_test_vect = vectorization(captions_test)"
      ],
      "metadata": {
        "id": "gs6MgAk7_TUZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP.compile(optimizer=optimizer, loss=CLIP_loss_fn)\n",
        "\n",
        "history = CLIP.fit(\n",
        "    x=[images_train, captions_train_vect],\n",
        "    y=tf.zeros(len(train_idx),2),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    #callbacks = [\n",
        "    #    tfk.callbacks.EarlyStopping(mode='max', patience=5, restore_best_weights=True)\n",
        "    #]\n",
        "    validation_split = 0.2\n",
        ")"
      ],
      "metadata": {
        "id": "zjrl1WOxpaJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7iBjDRFH63Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell we execute the garbage collector because sometimes there is some garbage resulting from the training that occupies precious RAM memory."
      ],
      "metadata": {
        "id": "uCpFHYZt06k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4UljyfpJqEoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}